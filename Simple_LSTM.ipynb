{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev3 toc-item\"><a href=\"#Train-a-simple-LSTM\" data-toc-modified-id=\"Train-a-simple-LSTM-001\"><span class=\"toc-item-num\">0.0.1&nbsp;&nbsp;</span>Train a simple LSTM</a></div><div class=\"lev3 toc-item\"><a href=\"#Load-the-final-weigths-of-the-final-LSTM-tuned\" data-toc-modified-id=\"Load-the-final-weigths-of-the-final-LSTM-tuned-002\"><span class=\"toc-item-num\">0.0.2&nbsp;&nbsp;</span>Load the final weigths of the final LSTM tuned</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a simple LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.5/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n",
    "from keras.layers import Concatenate, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import sys\n",
    "\n",
    "from time import time\n",
    "import gc\n",
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "from gensim.models import word2vec\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test_snello_pulito.csv', encoding='latin1')\n",
    "train_df = pd.read_csv('train_snello_pulito.csv', encoding='latin1')\n",
    "train_df.fillna(\"ciccia\", inplace=True)\n",
    "q1, q2 = 'question1_final', 'question2_final'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 30\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 135\n",
    "VALIDATION_SPLIT = 0.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_corpus(data, q1, q2):\n",
    "    \"Creates a list of lists containing words from each sentence\"\n",
    "    corpus = []\n",
    "    for col in [q1, q2]:\n",
    "        for sentence in data[col].iteritems():\n",
    "            word_list = sentence[1].split(\" \")\n",
    "            corpus.append(word_list)\n",
    "\n",
    "    return corpus\n",
    "\n",
    "\n",
    "corpus = build_corpus(train_df, q1, q2)\n",
    "corpus.extend(build_corpus(test_df, q1, q2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(corpus, size=EMBEDDING_DIM, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_DIR = ''\n",
    "EMBEDDING_FILE = BASE_DIR + 'model_best.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec = KeyedVectors.load(EMBEDDING_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts_1 = list(train_df.question1_final.values)\n",
    "texts_2 = list(train_df.question2_final.values)\n",
    "test_texts_1 = list(test_df.question1_final.values)\n",
    "test_texts_2 = list(test_df.question2_final.values)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts_1 + texts_2 + test_texts_1 + test_texts_2)\n",
    "\n",
    "sequences_1 = tokenizer.texts_to_sequences(texts_1)\n",
    "sequences_2 = tokenizer.texts_to_sequences(texts_2)\n",
    "test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n",
    "test_sequences_2 = tokenizer.texts_to_sequences(test_texts_2)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "data_1 = pad_sequences(sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_2 = pad_sequences(sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.array(train_df.is_duplicate.values)\n",
    "print('Shape of data tensor:', data_1.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "test_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data_2 = pad_sequences(test_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_words = min(MAX_NB_WORDS, len(word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec.wv.vocab:\n",
    "        embedding_matrix[i] = word2vec.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('labels.gz', labels)\n",
    "np.savetxt('data_1.gz', data_1)\n",
    "np.savetxt('data_2.gz', data_2)\n",
    "np.savetxt('embedding_matrix.gz', embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = np.loadtxt('labels.gz')\n",
    "data_1 = np.loadtxt('data_1.gz')\n",
    "data_2 = np.loadtxt('data_2.gz')\n",
    "embedding_matrix = np.loadtxt('embedding_matrix.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "perm = np.random.permutation(len(data_1))\n",
    "idx_train = perm[:int(len(data_1)*(1-VALIDATION_SPLIT))]\n",
    "idx_val = perm[int(len(data_1)*(1-VALIDATION_SPLIT)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_1_train = np.vstack((data_1[idx_train], data_2[idx_train]))\n",
    "data_2_train = np.vstack((data_2[idx_train], data_1[idx_train]))\n",
    "labels_train = np.concatenate((labels[idx_train], labels[idx_train]))\n",
    "\n",
    "data_1_val = np.vstack((data_1[idx_val], data_2[idx_val]))\n",
    "data_2_val = np.vstack((data_2[idx_val], data_1[idx_val]))\n",
    "labels_val = np.concatenate((labels[idx_val], labels[idx_val]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_val = np.ones(len(labels_val))\n",
    "weight_val *= 0.472001959\n",
    "weight_val[labels_val==0] = 1.309028344"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU is related to LSTM as both are utilizing different way if gating information to prevent vanishing gradient problem. \n",
    "\n",
    "Here are some pin-points about GRU vs LSTM:\n",
    "- The GRU unit controls the flow of information like the LSTM unit, but without having to use a memory unit. It just exposes the full hidden content without any control.\n",
    "- GRU is relatively new, and from my perspective, the performance is on par with LSTM, but computationally more efficient(less complex structure as pointed out). So we are seeing it being used more and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(nb_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)\n",
    "\n",
    "lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm, return_sequences=True)\n",
    "gru_layer = GRU(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n",
    "\n",
    "sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "x1 = lstm_layer(embedded_sequences_1 )\n",
    "x2 = LSTM(num_lstm//2, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)(x1)\n",
    "\n",
    "sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "y1 = lstm_layer(embedded_sequences_2)\n",
    "y2 = LSTM(num_lstm//2, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)(y1)\n",
    "\n",
    "merged = concatenate([x2, y2])\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = Activation('sigmoid')(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "merged = Dense(num_dense, activation=act)(merged)\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "\n",
    "preds = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "class_weight = {0: 1.309028344, 1: 0.472001959}\n",
    "\n",
    "model = Model(inputs=[sequence_1_input, sequence_2_input], outputs=preds)\n",
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "print(model.summary())\n",
    "print(STAMP)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "bst_model_path = STAMP + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    bst_model_path, save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 646864 samples, validate on 161716 samples\n",
      "Epoch 1/10\n",
      "646864/646864 [==============================] - 237s - loss: 0.4576 - acc: 0.6584 - val_loss: 0.4331 - val_acc: 0.6382\n",
      "Epoch 2/10\n",
      "646864/646864 [==============================] - 235s - loss: 0.3939 - acc: 0.6755 - val_loss: 0.3930 - val_acc: 0.6499\n",
      "Epoch 3/10\n",
      "646864/646864 [==============================] - 235s - loss: 0.3805 - acc: 0.6855 - val_loss: 0.5614 - val_acc: 0.6304\n",
      "Epoch 4/10\n",
      "646864/646864 [==============================] - 235s - loss: 0.3733 - acc: 0.6908 - val_loss: 0.3663 - val_acc: 0.6906\n",
      "Epoch 5/10\n",
      "646864/646864 [==============================] - 235s - loss: 0.3638 - acc: 0.7006 - val_loss: 0.3632 - val_acc: 0.6913\n",
      "Epoch 6/10\n",
      "646864/646864 [==============================] - 235s - loss: 0.3595 - acc: 0.7049 - val_loss: 0.4135 - val_acc: 0.7355\n",
      "Epoch 7/10\n",
      "646864/646864 [==============================] - 235s - loss: 0.3540 - acc: 0.7109 - val_loss: 0.3456 - val_acc: 0.7189\n",
      "Epoch 8/10\n",
      "646864/646864 [==============================] - 235s - loss: 0.3486 - acc: 0.7160 - val_loss: 0.3519 - val_acc: 0.6946\n",
      "Epoch 9/10\n",
      "646864/646864 [==============================] - 235s - loss: 0.3444 - acc: 0.7206 - val_loss: 0.3358 - val_acc: 0.7338\n",
      "Epoch 10/10\n",
      "646864/646864 [==============================] - 235s - loss: 0.3400 - acc: 0.7250 - val_loss: 0.3419 - val_acc: 0.7371\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(\n",
    "    [data_1_train, data_2_train],\n",
    "    labels_train,\n",
    "    validation_data=([data_1_val, data_2_val], labels_val, weight_val),\n",
    "    epochs=10,\n",
    "    batch_size=4092,\n",
    "    shuffle=True,\n",
    "    class_weight=class_weight,\n",
    "    callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "model.load_weights(bst_model_path)\n",
    "bst_val_score = min(hist.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Load the final weigths of the final LSTM tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_lstm = 210 # int(np.random.randint(175, 230)) # 175-275\n",
    "rate_drop_lstm = 0.3 # 0.15 + np.random.rand() * 0.25\n",
    "num_dense = 135 # np.random.randint(100, 150)\n",
    "class_weight = {0:1, 1:0.282} #{0: r*rtrain, 1: 1}# {0: 1.309028344, 1: 0.472001959}\n",
    "firts_lstm={'num_lstm':num_lstm, 'rate_drop_lstm':rate_drop_lstm}\n",
    "activation='relu'\n",
    "drop_rate=0.3\n",
    "dropout = 0.3\n",
    "recurrent_dropout=0.3\n",
    "rate_drop_lstm = firts_lstm['rate_drop_lstm']\n",
    "\n",
    "\n",
    "embedding_layer = Embedding(nb_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)\n",
    "lstm_layer = LSTM(firts_lstm['num_lstm'], dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm, \n",
    "                  return_sequences=True)\n",
    "\n",
    "sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "    \n",
    "x = lstm_layer(embedded_sequences_1)\n",
    "y = lstm_layer(embedded_sequences_2)\n",
    "x = LSTM(num_lstm//2, dropout=dropout, recurrent_dropout=recurrent_dropout)(x)\n",
    "y = LSTM(num_lstm//2, dropout=dropout, recurrent_dropout=recurrent_dropout)(y)\n",
    "\n",
    "x = Dropout(drop_rate)(x)\n",
    "y = Dropout(drop_rate)(y)\n",
    "merged = concatenate([x, y])\n",
    "merged = Dropout(drop_rate)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "\n",
    "merged = Dense(num_dense, activation=activation)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dropout(drop_rate)(merged)        \n",
    "        \n",
    "merged = Dense(num_dense, activation=activation)(merged)\n",
    "merged = Dropout(drop_rate)(merged)\n",
    "merged = BatchNormalization()(merged)    \n",
    "preds = Dense(1, activation='sigmoid')(merged)\n",
    "    \n",
    "model = Model(inputs=[sequence_1_input, sequence_2_input], outputs=preds)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created model and loaded weights from file\n"
     ]
    }
   ],
   "source": [
    "# load weights\n",
    "model.load_weights(\"lstm_dense_dropout_rmsprop_True_relu_2048_0.3_210_0.3.h5\")\n",
    "# Compile model (required to make predictions)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(\"Created model and loaded weights from file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('./lstm_dense_dropout_rmsprop_True_relu_2048_0.3_210_0.3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ids = test_df['test_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = model.predict([test_data_1, test_data_2], batch_size=8192, verbose=1)\n",
    "preds += model.predict([test_data_2, test_data_1], batch_size=8192, verbose=1)\n",
    "preds /= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'test_id':test_ids, 'is_duplicate':preds.ravel()})\n",
    "submission.to_csv('submissionNN.csv'', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "63px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
